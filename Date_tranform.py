import numpy as np # å¯¼å…¥æ•°å­¦è®¡ç®—çš„å·¥å…·
import re          # å¯¼å…¥å¤„ç†æ–‡æœ¬ï¼ˆæ‰¾è§„å¾‹ï¼‰çš„å·¥å…·
import pandas as pd # å¯¼å…¥å¤„ç†è¡¨æ ¼æ•°æ®çš„å·¥å…· (æ–°å¢)

def parse_plt_to_tensor(file_list, output_csv_name='extracted_id_data.csv'):
    """
    å‡½æ•°åŠŸèƒ½ï¼šè§£æ .plt æ–‡ä»¶ï¼Œæå–ç”µæµ (Id) æ•°æ®ï¼Œè¿›è¡Œå½’ä¸€åŒ–ï¼Œ
            å¹¶è½¬æ¢æˆç¥ç»ç½‘ç»œéœ€è¦çš„å¼ é‡æ ¼å¼ã€‚æœ€åä¿å­˜å½’ä¸€åŒ–æ•°æ®åˆ° CSV æ–‡ä»¶ã€‚
            
    file_listï¼šè¦å¤„ç†çš„æ–‡ä»¶ååˆ—è¡¨ã€‚
    output_csv_nameï¼šè¦ä¿å­˜çš„ CSV æ–‡ä»¶åã€‚
    """
    
    all_device_data = [] 
    
    # ------------------ (1-4 æ­¥ï¼šæ•°æ®æå–) ------------------
    for file_path in file_list:
        try:
            with open(file_path, 'r') as f:
                content = f.read()
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{file_path}'ï¼Œè¯·ç¡®ä¿æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹ã€‚")
            continue
            
        data_match = re.search(r'Data\s*\{([\s\S]*)\}', content)
        if not data_match:
            print(f"âš ï¸ æ–‡ä»¶ '{file_path}' ä¸­æ‰¾ä¸åˆ° 'Data' å—ï¼Œè·³è¿‡æ­¤æ–‡ä»¶ã€‚")
            continue
            
        raw_values = re.findall(r'[-+]?\d*\.\d+[eE][-+]?\d+', data_match.group(1))
        float_values = [float(v) for v in raw_values]
        
        num_variables = 32 
        num_points = len(float_values) // num_variables 
        
        id_curve = [] 
        for i in range(num_points):
            # Id æ˜¯ç¬¬ 31 ä¸ªå˜é‡ (ç´¢å¼• 30)
            try:
                 id_val = float_values[i * num_variables + 30] 
                 id_curve.append(id_val)
            except IndexError:
                 print(f"âš ï¸ æ–‡ä»¶ '{file_path}' æ•°æ®ç‚¹ä¸è¶³æˆ–æ ¼å¼é”™è¯¯ã€‚")
                 break

        if id_curve:
             all_device_data.append(id_curve) 
        else:
             print(f"âŒ æ–‡ä»¶ '{file_path}' æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„ Id æ›²çº¿æ•°æ®ã€‚")
             
    # ------------------ (5 æ­¥ï¼šæ•´ç†å’Œå½’ä¸€åŒ–) ------------------
    if not all_device_data:
        print("âŒ æ— æ³•åˆ›å»ºå¼ é‡ï¼šæ²¡æœ‰ä»æ–‡ä»¶ä¸­æå–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ã€‚")
        return None, None
        
    data_array = np.array(all_device_data) 
    
    # å½’ä¸€åŒ–å¤„ç†
    data_min = data_array.min()  
    data_max = data_array.max()  
    
    # é¿å…é™¤ä»¥é›¶ï¼šå¦‚æœæ‰€æœ‰ç”µæµå€¼éƒ½ç›¸åŒ (å³ min == max)ï¼Œåˆ™å½’ä¸€åŒ–ä¸ºå…¨é›¶æˆ–å…¨ä¸€ã€‚
    if data_max == data_min:
         normalized_data = np.zeros_like(data_array)
         print("â„¹ï¸ è­¦å‘Šï¼šæ‰€æœ‰ç”µæµå€¼ç›¸åŒï¼Œå½’ä¸€åŒ–ç»“æœä¸ºé›¶ã€‚")
    else:
         normalized_data = (data_array - data_min) / (data_max - data_min)
    
    # ------------------ (6 æ­¥ï¼šä¿å­˜ CSV æ–‡ä»¶) ------------------
    
    # åˆ›å»ºè¡¨æ ¼ï¼Œä»¥ä¾¿ä¿å­˜åˆ° CSV
    # æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå™¨ä»¶çš„ Id æ›²çº¿ï¼Œæ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªé‡‡æ ·ç‚¹
    column_names = [f'Point_{i+1}' for i in range(normalized_data.shape[1])]
    row_index = [f'{file.split(".")[0]}' for file in file_list if file.split(".")[0] in ['Id-Vds23n23_des', 'Id-Vds26n26_des', 'Id-Vds27n27_des']]
    
    df = pd.DataFrame(normalized_data, index=row_index, columns=column_names)
    df.to_csv(output_csv_name)
    
    print(f"ğŸ‰ å½’ä¸€åŒ–åçš„æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°æ–‡ä»¶: {output_csv_name}")
    print("\n--- CSV æ–‡ä»¶å†…å®¹é¢„è§ˆ ---")
    print(df.head())
    
    # ------------------ (7 æ­¥ï¼šè°ƒæ•´å¼ é‡å½¢çŠ¶) ------------------
    # æœ€ç»ˆå½¢çŠ¶ (1, å™¨ä»¶æ•°, æ›²çº¿æ•°, é‡‡æ ·ç‚¹æ•°) -> (1, 3, 1, 20)
    final_tensor = normalized_data.reshape(1, normalized_data.shape[0], 1, normalized_data.shape[1])
    
    return final_tensor, df

# --- ä½¿ç”¨ç¨‹åº ---

# è®¾å®šä½ è¦å¤„ç†çš„æ–‡ä»¶åå­—
# å‡è®¾æ‚¨å·²å°†æ–‡ä»¶åä¿®æ”¹ä¸º .txt æˆ–ç¨‹åºèƒ½ç›´æ¥è¯»å– .plt æ–‡ä»¶
files = ['Id-Vds23n23_des.plt', 'Id-Vds26n26_des.plt', 'Id-Vds27n27_des.plt']

# è¿è¡Œå‡½æ•°ï¼Œè·å–æœ€ç»ˆçš„ç¥ç»ç½‘ç»œè¾“å…¥æ•°æ®ï¼Œå¹¶ä¿å­˜ CSV
input_tensor, data_frame = parse_plt_to_tensor(files)

if input_tensor is not None:
    print("\nâœ… æ•°æ®æå–å’Œä¿å­˜å®Œæˆï¼")
    print("æå–åçš„å¼ é‡å½¢çŠ¶ (å°±æ˜¯æ•°æ®çŸ©é˜µçš„è§„æ ¼):", input_tensor.shape)